%%clcclear allclose all%Lamrous PCA program on notes[dataNum, datatext, alldata] = xlsread('voiture.xls','voiture2');Data=dataNum(:,1:7);Variables = datatext(1,2:8)Individuals = datatext(2:31,1)%This allows you to generate a PCA%Attention, in the latest versions of Matlab, the "princomp" function is%called otherwise,try "pca" %% question 5% 5) Unroll the Clustering Kmeans algorithm on the global matrix with the following parameters:% - Initialization: uniform% - Number of groups: 2% - Number of replications: 1 [idx,centres]=kmeans(Data,2,'Replicates',1,'Start','uniform');%% question 5 % Is your algorithm stable from one execution to another? If "no" in what condition would it be?% Our algorithm is unstable because at each iteration the center is chosen randomly. % the code suggested below manually sets the center for clustering[dataNum, datatext, alldata] = xlsread('voiture.xls','voiture2');DataKmeans=dataNum(:,1:7);%for i=1:10%fixedCenter = randi([1 5],2,7)fixedCenter = rand(2,7)[idx,centres]=kmeans(DataKmeans,2,'Replicates',1,'Start',fixedCenter);figure;plot(DataKmeans(idx==1,1),DataKmeans(idx==1,2),'r.','MarkerSize',12)hold onplot(DataKmeans(idx==2,1),DataKmeans(idx==2,2),'b.','MarkerSize',12)plot(centres(:,1),centres(:,2),'kx',...     'MarkerSize',15,'LineWidth',3) legend('Cluster 1','Cluster 2','Centroids',...       'Location','NW')title 'Cluster without PCA'text(DataKmeans(:,1),DataKmeans(:,2),Individuals); hold off% end%% question 6 % What is the best grouping? % answer: 2% see how does it work here: https://fr.mathworks.com/help/stats/evalclusters.htmleva = evalclusters(Data,'kmeans','CalinskiHarabasz','KList',[1:2]..."maximum clustering can be 30, but we should work with two"     )%%[coeff,scores,eigen_values] = pca(zscore(Data)); coeff % coordinates of variables in the projection space scores % coordinates of individuals in the projection space eigen_values % Eigenvalues%The following command is used to create the graph of the eigenvalues.figure('Name','Eigenvalue spectrum','NumberTitle','off'); bar(eigen_values); % And this gives us the cumulative inertias of the eigenvalues. Information_Percentage = cumsum(eigen_values)./sum(eigen_values)figure('Name','% of cumulative inertia','NumberTitle','off'); bar(Information_Percentage); % This command is used to produce the factorial map of individuals on% the two main axes figure('Name','individus','NumberTitle','off'); plot(scores(:,1),scores(:,2),'r+');text(scores(:,1),scores(:,2),Individuals); a  = axis;xl = a(1);xu = a(2);yl = a(3);yu = a(4);xlabel('1st Principal Component')ylabel('2nd Principal Component')hold online([xl xu],[0 0])line([0 0],[yl yu])%axis 1 axis 3figure('Name','individuals 1/3','NumberTitle','off'); plot(scores(:,1),scores(:,3),'r+');text(scores(:,1),scores(:,3),Individuals); a  = axis;xl = a(1);xu = a(2);yl = a(3);yu = a(4);xlabel('1st Principal Component')ylabel('2nd Principal Component')hold online([xl xu],[0 0])line([0 0],[yl yu])%axis 2 axis 3figure('Name','individuals 2/3','NumberTitle','off'); plot(scores(:,1),scores(:,3),'r+');text(scores(:,1),scores(:,3),Individuals); a  = axis;xl = a(1);xu = a(2);yl = a(3);yu = a(4);xlabel('1st Principal Component')ylabel('2nd Principal Component')hold online([xl xu],[0 0])line([0 0],[yl yu])% In 3Dfigure('Name','individuals 1 2 3','NumberTitle','off'); plot3(scores(:,1),scores(:,2),scores(:,3),'r+');text(scores(:,1),scores(:,2),scores(:,3),Individuals); a  = axis;xl = a(1);xu = a(2);yl = a(3);yu = a(4);z1=a(5);zu=a(6);xlabel('1st Principal Component')ylabel('2nd Principal Component')zlabel('3nd Principal Component')hold online([xl xu],[0, 0],[0, 0])line([0 0],[yl yu],[0,0])line([0 0],[0,0],[z1 zu])% This command is used to create the circle of variable correlations% on the two main axes. figure('Name','Variables','NumberTitle','off'); plot(coeff(:,1),coeff(:,2),'*');%% question 7% Unroll your algorithm on the results of a PCA with at least 94% of information.% What is your observation?% % We got from Eigenvalue spectrum figure that 70 is a 100% percent% its mean that infomation which is more then 94% containts in first 3% splash, thats why we calculate for 65.8 as 94%% of the information% after PCA we got 0.6905 CriterionValues using SilhouetteEvaluation with% kmeans of 2 groups which was 0.6636 with the same without PCA. It's been% concluded that after PCA if we apply kmeans then it provides better quality with clustering. fixedCenter = rand(2,3)% Retain first three principal componentsselectedThree = scores(:,1:3);[idx,centres]=kmeans(selectedThree,2,'Replicates',1,'Start',fixedCenter);figure;plot(selectedThree(idx==1,1),selectedThree(idx==1,2),'r.','MarkerSize',12)hold onplot(selectedThree(idx==2,1),selectedThree(idx==2,2),'b.','MarkerSize',12)plot(centres(:,1),centres(:,2),'kx',...     'MarkerSize',15,'LineWidth',3) legend('Cluster 1','Cluster 2','Centroids',...       'Location','NW')title 'Cluster with KMeans and almost 94% information from PCA'text(scores(:,1),scores(:,2),Individuals); hold offeva = evalclusters(Data,'kmeans','Silhouette','KList',[1:2]..."maximum clustering can be 30, but we should work with two"     )eva = evalclusters(selectedThree,'kmeans','Silhouette','KList',[1:2]..."maximum clustering can be 30, but we should work with two"     )